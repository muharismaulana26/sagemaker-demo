{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating Jobs, Model Registration, and Continuous Deployment with Amazon SageMaker\n",
    "\n",
    "Amazon SageMaker offers Machine Learning application developers and Machine Learning operations engineers the ability to orchestrate SageMaker jobs and author reproducible Machine Learning pipelines, deploy custom-build models for inference in real-time with low latency or offline inferences with Batch Transform, and track lineage of artifacts. You can institute sound operational practices in deploying and monitoring production workflows, deployment of model artifacts, and track artifact lineage through a simple interface, adhering to safety and best-practice paradigmsfor Machine Learning application development.\n",
    "\n",
    "The SageMaker Pipelines service supports a SageMaker Machine Learning Pipeline Domain Specific Language (DSL), which is a declarative Json specification. This DSL defines a Directed Acyclic Graph (DAG) of pipeline parameters and SageMaker job steps. The SageMaker Python Software Developer Kit (SDK) streamlines the generation of the pipeline DSL using constructs that are already familiar to engineers and scientists alike.\n",
    "\n",
    "The SageMaker Model Registry is where trained models are stored, versioned, and managed. Data Scientists and Machine Learning Engineers can compare model versions, approve models for deployment, and deploy models from different AWS accounts, all from a single Model Registry. SageMaker enables customers to follow the best practices with ML Ops and getting started right. Customers are able to standup a full ML Ops end-to-end system with a single API call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SageMaker Pipelines\n",
    "\n",
    "Amazon SageMaker Pipelines support the following activites:\n",
    "\n",
    "* Pipelines - A Directed Acyclic Graph of steps and conditions to orchestrate SageMaker jobs and resource creation.\n",
    "* Processing Job steps - A simplified, managed experience on SageMaker to run data processing workloads, such as feature engineering, data validation, model evaluation, and model interpretation.\n",
    "* Training Job steps - An iterative process that teaches a model to make predictions by presenting examples from a training dataset.\n",
    "* Conditional step execution - Provides conditional execution of branches in a pipeline.\n",
    "* Registering Models - Creates a model package resource in the Model Registry that can be used to create deployable models in Amazon SageMaker.\n",
    "* Creating Model steps - Create a model for use in transform steps or later publication as an endpoint.\n",
    "* Parameterized Pipeline executions - Allows pipeline executions to vary by supplied parameters.\n",
    "* Transform Job steps - A batch transform to preprocess datasets to remove noise or bias that interferes with training or inference from your dataset, get inferences from large datasets, and run inference when you don't need a persistent endpoint.\n",
    "* Callback Step - A step that allows users to invoke jobs outside of SageMaker from the Pipeline\n",
    "* Lambda Step - A step that allows users to invoke a Lambda function from the Pipeline\n",
    "* Quality Check Step - A step that runs Model Quality or Data Quality checks on a dataset or model. Baselines are created by this step that can be associated with the model when it is added to the Model Registry. \n",
    "* Clarify Check Step - A step that uses SageMaker Clarify to find bias and explainability metrics on a model or dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Layout of the SageMaker ModelBuild Project Template\n",
    "\n",
    "The template provides a starting point for bringing your SageMaker Pipeline development to production.\n",
    "\n",
    "```\n",
    "|-- codebuild-buildspec.yml\n",
    "|-- CONTRIBUTING.md\n",
    "|-- pipelines\n",
    "|   |-- abalone\n",
    "|   |   |-- evaluate.py\n",
    "|   |   |-- __init__.py\n",
    "|   |   |-- pipeline.py\n",
    "|   |   `-- preprocess.py\n",
    "|   |-- get_pipeline_definition.py\n",
    "|   |-- __init__.py\n",
    "|   |-- run_pipeline.py\n",
    "|   |-- _utils.py\n",
    "|   `-- __version__.py\n",
    "|-- README.md\n",
    "|-- sagemaker-pipelines-project.ipynb\n",
    "|-- setup.cfg\n",
    "|-- setup.py\n",
    "|-- tests\n",
    "|   `-- test_pipelines.py\n",
    "`-- tox.ini\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A description of some of the artifacts is provided below:\n",
    "<br/><br/>\n",
    "Your codebuild execution instructions:\n",
    "```\n",
    "|-- codebuild-buildspec.yml\n",
    "```\n",
    "<br/><br/>\n",
    "Your pipeline artifacts, which includes a pipeline module defining the required `get_pipeline` method that returns an instance of a SageMaker pipeline, a preprocessing script that is used in feature engineering, and a model evaluation script to measure the Mean Squared Error of the model that's trained by the pipeline:\n",
    "\n",
    "```\n",
    "|-- pipelines\n",
    "|   |-- abalone\n",
    "|   |   |-- evaluate.py\n",
    "|   |   |-- __init__.py\n",
    "|   |   |-- pipeline.py\n",
    "|   |   `-- preprocess.py\n",
    "\n",
    "```\n",
    "\n",
    "For additional subfolders with code and/or artifacts needed by pipeline, they need to be packaged correctly by the `setup.py` file. For example, to package a `pipelines/source` folder,\n",
    "\n",
    "* Include a `__init__.py` file within the `source` folder.\n",
    "* Add it to the `setup.py` file's `package_data` like so:\n",
    "\n",
    "```\n",
    "...\n",
    "    packages=setuptools.find_packages(),\n",
    "    include_package_data=True,\n",
    "    package_data={\"pipelines.my_pipeline.src\": [\"*.txt\"]},\n",
    "    python_requires=\">=3.6\",\n",
    "    install_requires=required_packages,\n",
    "    extras_require=extras,\n",
    "...\n",
    "```\n",
    "\n",
    "<br/><br/>\n",
    "Utility modules for getting pipeline definition jsons and running pipelines:\n",
    "\n",
    "```\n",
    "|-- pipelines\n",
    "|   |-- get_pipeline_definition.py\n",
    "|   |-- __init__.py\n",
    "|   |-- run_pipeline.py\n",
    "|   |-- _utils.py\n",
    "|   `-- __version__.py\n",
    "```\n",
    "<br/><br/>\n",
    "Python package artifacts:\n",
    "```\n",
    "|-- setup.cfg\n",
    "|-- setup.py\n",
    "```\n",
    "<br/><br/>\n",
    "A stubbed testing module for testing your pipeline as you develop:\n",
    "```\n",
    "|-- tests\n",
    "|   `-- test_pipelines.py\n",
    "```\n",
    "<br/><br/>\n",
    "The `tox` testing framework configuration:\n",
    "```\n",
    "`-- tox.ini\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A SageMaker Pipeline\n",
    "\n",
    "The pipeline that we create follows a typical Machine Learning Application pattern of pre-processing, training, evaluation, and conditional model registration and publication, if the quality of the model is sufficient.\n",
    "\n",
    "![A typical ML Application pipeline](img/mm-clarify-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data/Model Quality, Bias, and Model Explainability Checks in SageMaker Pipelines\n",
    "\n",
    "This notebook introduces two new step types in SageMaker Pipelines - \n",
    "* `QualityCheckStep`\n",
    "* `ClarifyCheckStep`\n",
    "\n",
    "With these two steps, the pipeline is able to perform baseline calculations that are needed as a standard against which data/model quality issues can be detected (including bias and explainability).\n",
    "\n",
    "These steps leverage SageMaker pre-built containers:\n",
    "\n",
    "* `QualityCheckStep` (for Data/Model Quality): [sagemaker-model-monitor-analyzer](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-pre-built-container.html)\n",
    "* `ClarifyCheckStep` (for Data/Model Bias and Model Explainability): [sagemaker-clarify-processing](https://docs.aws.amazon.com/sagemaker/latest/dg/clarify-configure-processing-jobs.html#clarify-processing-job-configure-container)\n",
    "\n",
    "The training dataset that you used to trained the model is usually a good baseline dataset. The training dataset data schema and the inference dataset schema should exactly match (the number and order of the features). Note that the prediction/output columns are assumed to be the first columns in the training dataset. From the training dataset, you can ask SageMaker to suggest a set of baseline constraints and generate descriptive statistics to explore the data.\n",
    "\n",
    "These two new steps will always calculate new baselines using the dataset provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drift Check Baselines in the Model Registry\n",
    "\n",
    "The `RegisterStep` has a new parameter called `drift_check_baselines`. This refers to the baseline files associated with the model. When deployed, these baseline files are used by Model Monitor for Model Quality/Data Quality checks. In addition, these baselines can be used in `QualityCheckStep` and `ClarifyCheckStep` to compare newly trained models against models that have already been registered in the Model Registry. \n",
    "\n",
    "### Step Properties\n",
    "\n",
    "The `QualityCheckStep` has the following properties -\n",
    "\n",
    "* `CalculatedBaselineStatistics` : The baseline statistics file calculated by the underlying Model Monitor container.\n",
    "* `CalculatedBaselineConstraints` : The baseline constraints file calculated by the underlying Model Monitor container. \n",
    "* `BaselineUsedForDriftCheckStatistics` and `BaselineUsedForDriftCheckConstraints` : These are the two properties used to set `drift_check_baseline` in the Model Registry. The values set in these properties vary depending on the parameters passed to the step. The different behaviors are described in the table below. \n",
    "  \n",
    "The `ClarifyCheckStep` has the following properties - \n",
    "\n",
    "* `CalculatedBaselineConstraints` : The baseline constraints file calculated by the underlying Clarify container. \n",
    "* `BaselineUsedForDriftCheckConstraints` : This property is used to set `drift_check_baseline` in the Model Registry. The values set in this property will vary depending on the parameters passed to the step. The different behaviors are described in the table below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting some constants\n",
    "\n",
    "We get some constants from the local execution environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = ''\n",
    "\n",
    "# Change these to reflect your project/business name or if you want to separate ModelPackageGroup/Pipeline from the rest of your team\n",
    "model_package_group_name = ''\n",
    "pipeline_name = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the pipeline instance\n",
    "\n",
    "Here we get the pipeline instance from your pipeline module so that we can work with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline_context.py:261: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "from pipelines.forecast.pipeline import get_pipeline\n",
    "\n",
    "\n",
    "pipeline = get_pipeline(\n",
    "    region=region,\n",
    "    role=role,\n",
    "    default_bucket=default_bucket,\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    pipeline_name=pipeline_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Calculation and Registration \n",
    "\n",
    "Both `QualityCheckStep` and `ClarifyCheckStep` will always calculate a new baseline for a certain check type given required inputs, and the newly calculated baselines are accessible through the step properties `CalculatedBaselineConstraints` and `CalculatedBaselineStatistics`. \n",
    "\n",
    "**`ClarifyCheckStep` only generated constraints and not statistics so the properties available on the `ClarifyCheckStep` are `BaselineUsedForDriftCheckConstraints` and `CalculatedBaselineConstraints`**\n",
    "\n",
    "*Note that baselines suggested by pre-built containers could be very strict, and it is recommended to inspect and adjust suggested baselines before using for drift checks.*\n",
    "\n",
    "Both `QualityCheckStep` and `ClarifyCheckStep` use two boolean flags `skip_check` and `register_new_baseline` to control their behavior. \n",
    "\n",
    "* `skip_check` : This determines if a drift check is executed or not.\n",
    "* `register_new_baseline` : This determines if the newly calculated baselines (in the step property `CalculatedBaselines`) should be set in the step property `BaselineUsedForDriftCheck`.\n",
    "* `supplied_baseline_statistics` and `supplied_baseline_constraints` : If skip_check is set to False, baselines can be provided to this step through this parameter. If provided, the step will compare the newly calculated baselines (`CalculatedBaselines`) against those provided here instead of finding the latest baselines from the Model Registry. In the case of `ClarifyCheckStep`, only `supplied_baseline_constraints` is a valid parameter, for `QualityCheckStep`, both parameters are used. \n",
    "* `model_package_group_name` : The step will use the `drift_check_baselines` from the latest approved model in the model package group for the drift check. If `supplied_baseline_*` is provided, this field will be ignored.\n",
    "\n",
    "The first time the pipeline is run, the `skip_check` value should be set to True using the pipeline execution parameters so that new baselines are registered and no drift check is executed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the pipeline we get above, these flags of all the check steps are exposed as pipeline parameters for convenience. The default values of this two flags of all the QualityCheck and ClarifyCheck steps in the pipeline instance we used above are defined as `False`, which indicates in our pipeline runs, we will enable all the drift checks, and previous baseline in each step is assessible through step property `BaselineUsedForDriftCheckConstraints` (and `BaselineUsedForDriftCheckStatistics` if it is a QualtiyCheck step). However, for the initial run we will override them all to `True` since we want to have our initial calculated baselines registered as the baseline for drift check in Model Registry. Details of the flag values and step behaviors will be covered later.\n",
    "\n",
    "In the RegisterModel step, `BaselineUsedForDriftCheckConstraints` (and `BaselineUsedForDriftCheckStatistics` if it is a `QualtiyCheckStep`) of all QualityCheck and ClarifyCheck steps are registered as `drift_check_baselines` in the new model package created for future drift check purpose, while `CaculatedBaselineConstraints` (and `CalculatedBaselineStatistics` if it is a `QualityCheckStep`) are registered as `ModelMetrics` in the new model package for auditing purpose.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the pipeline to SageMaker and start execution\n",
    "\n",
    "Let's submit our pipeline definition to the workflow service. The role passed in will be used by the workflow service to create all the jobs defined in the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start the pipeline, values can be passed into these pipeline parameters on starting of the pipeline. Here we override all `skip_check` and `register_new_baseline` to `True` for the initial run as mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set pipeline parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| `skip_check` / `register_new_baseline` | Does step do a drift check?                              | Value of step property `CalculatedBaseline`                                                           | Value of step property `BaselineUsedForDriftCheck`      | Possible Circumstances for this parameter combination|\n",
    "| -------------------------------------- | ---------------------------------------------------------|------------------------------------------------------------                                           |------------------------------------------------- | -----------------------------------------------------| \n",
    "| T / T                                  | No Drift Check.                                          | New baselines calculated by step execution                  | Newly calculated baseline by step execution (value of property `CalculatedBaseline`)                                     | a. Initial run of the pipeline, building the first model version and generate initial baselines. <br>b. Violation detected by the model monitor on endpoint for a particular type of check and the pipeline is triggered for retraining a new model. Skip the check against previous baselines and refresh DriftCheckBaselines with newly calculated baselines in Registry directly.  |\n",
    "| F / F                                  | Drift Check executed against existing baselines.         | New baselines calculated by step execution                                                            |  Baseline from latest approved model in Model Registry or baseline supplied as step parameter                                    | Regular re-training with checks enabled to get a new model version, but carry over previous baselines as DriftCheckBaselines in Registry for new model version.                                                                                                                                                             |\n",
    "| F / T                                  | Drift Check executed against existing baselines.         | New baselines calculated by step execution                  | Newly calculated baseline by step execution (value of property `CalculatedBaseline`)                                     | Regular re-training with checks enabled to get a new model version, but refresh DriftCheckBaselines in Registry with newly calculated baselines for the new model version.                                                                                                                                                  |\n",
    "| T / F                                  | No Drift Check.                                          | New baselines calculated by step execution          | Baseline from latest approved model in Model Registry or baseline supplied as step parameter                                     | Violation detected by the model monitor on endpoint for a particular type of check and the pipeline is triggered for retraining a new model. Skip the check against previous baselines, but carry over previous baselines as DriftCheckBaselines in Registry for new model version.                                                                                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default parameters are overriden the first time the pipeline is executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start(\n",
    "    parameters=dict(\n",
    "        SkipDataQualityCheck=True,\n",
    "        RegisterNewDataQualityBaseline=True,\n",
    "        SkipDataBiasCheck=True,\n",
    "        RegisterNewDataBiasBaseline=True,\n",
    "        SkipModelQualityCheck=True,\n",
    "        RegisterNewModelQualityBaseline=True,\n",
    "        SkipModelBiasCheck=True,\n",
    "        RegisterNewModelBiasBaseline=True,\n",
    "        SkipModelExplainabilityCheck=True,\n",
    "        RegisterNewModelExplainabilityBaseline=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsequent runs\n",
    "\n",
    "Once the baselines have been created for the first time, the default parameters of the pipeline can be used on subsequent runs so that new baselines are compared against old ones if model version approved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Operations: examining and waiting for pipeline execution\n",
    "\n",
    "Now we describe execution instance and list the steps in the execution to find out more about the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:302546992452:pipeline/forecastpipeline-example',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:us-east-1:302546992452:pipeline/forecastpipeline-example/execution/o2rg7jppfrxh',\n",
       " 'PipelineExecutionDisplayName': 'execution-1671376198545',\n",
       " 'PipelineExecutionStatus': 'Executing',\n",
       " 'CreationTime': datetime.datetime(2022, 12, 18, 15, 9, 58, 447000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2022, 12, 18, 15, 9, 58, 447000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:302546992452:user-profile/d-4qizv7vwhotp/glair-sagemaker-exploration',\n",
       "  'UserProfileName': 'glair-sagemaker-exploration',\n",
       "  'DomainId': 'd-4qizv7vwhotp'},\n",
       " 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:302546992452:user-profile/d-4qizv7vwhotp/glair-sagemaker-exploration',\n",
       "  'UserProfileName': 'glair-sagemaker-exploration',\n",
       "  'DomainId': 'd-4qizv7vwhotp'},\n",
       " 'ResponseMetadata': {'RequestId': '4d670ce7-d656-413a-8a53-b681b8650763',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '4d670ce7-d656-413a-8a53-b681b8650763',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '795',\n",
       "   'date': 'Sun, 18 Dec 2022 15:09:57 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can wait for the execution by invoking `wait()` on the execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "WaiterError",
     "evalue": "Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWaiterError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-72be0c8b7085>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexecution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, delay, max_attempts)\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0mwaiter_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         )\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPipelineExecutionArn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/waiter.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m# method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mWaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     wait.__doc__ = WaiterDocstring(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/botocore/waiter.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m                     \u001b[0mreason\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m                     \u001b[0mlast_response\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m                 )\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnum_attempts\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_attempts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWaiterError\u001b[0m: Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\""
     ]
    }
   ],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can list the execution steps to check out the status and artifacts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
